\documentclass{beamer}
\usepackage{mathtools}

\beamertemplatenavigationsymbolsempty{}

\title{Learning from Experts}
\subtitle{Expanding your Algorithmic Toolbox}
\date{Operations Research Tuesdays $\cdot$ March 17, 2015}

\author{Paul Beaujean\inst{\dag}}
\institute{\inst{\dag}IMT/OLN/NMP/TRM}

\begin{document}

\frame{\titlepage}

\begin{frame}
    \frametitle{Table of Contents}
    \tableofcontents{}
\end{frame}

\section{Learning from Experts}
\begin{frame}
    \frametitle{The Algorithm Designer}

    \begin{block}{Our usual toolkit}
        \begin{itemize}
            \item Linear Optimization
            \item Dynamic Programming
            \item Greedy Algorithms
            \item Metaheuristics\ldots
            \item Classification
            \item Clustering
        \end{itemize}
    \end{block}

    \begin{block}{Today}
        General algorithmic technique applicable to optimization, machine
        learning, and online decision making.
    \end{block}

\end{frame}

\begin{frame}
    \frametitle{Learning from Experts}

    \begin{block}{How can I apply this framework?}
        \begin{equation*}
            \mathcal{A} = \{ a~|~\ell_a^t \in [-1, 1]  \}
        \end{equation*}

        \begin{itemize}
            \item $a$ can be:
        \begin{itemize}
            \item a strategy
            \item an algorithm
            \item a prediction
        \end{itemize}
            \item $t$ is a round (discrete time)
        \end{itemize}
    \end{block}

\end{frame}

\begin{frame}
    \frametitle{Notation}

    \begin{block}{Action $a$ must have:}
        \begin{itemize}
            \item bounded loss: $\ell_a^t$
            \item bounded gain: $g_a^t$
        \end{itemize}
    \end{block}

    \begin{block}{Gain, Loss, Regret}
        \begin{itemize}
            \item Loss incurred by action $a$ over a certain number of rounds:
                \begin{equation*}
                    L_a = \sum_t \ell_a^t
                \end{equation*}
            \item Loss incurred by the best action in hindsight:
                \begin{equation*}
                    L_* = \min_{a\in\mathcal{A}} L_a
                \end{equation*}
        \end{itemize}
    \end{block}

\end{frame}

\begin{frame}
    \frametitle{A surprising result}

    For a certain algorithm with expected loss $L$ (or gain $G$):

    \begin{block}{Loss bound}
        \begin{equation*}
            L \leq L_a + \epsilon~|L_a| + \dfrac{\ln|\mathcal{A}|}{\epsilon},
            ~\forall a\in \mathcal{A}
        \end{equation*}
    \end{block}

    \begin{block}{Gain guarantee}
        \begin{equation*}
            G \geq G_a - \epsilon~|G_a| - \dfrac{\ln |\mathcal{A}|}{\epsilon},
            ~\forall a\in \mathcal{A}
        \end{equation*}
    \end{block}

\end{frame}

\subsection{The multiplicative weights update algorithm}
\begin{frame}
    \frametitle{Multiplicative weights}
    \framesubtitle{Compounding loss!}

    \begin{block}{Initialization}
        \begin{equation*}
            \lambda_a^1 = 1,~\forall a\in\mathcal{A}
        \end{equation*}
    \end{block}

    \begin{block}{Sequential Updates}
        \begin{itemize}
            \item Choose action $a$ with probability $p_a^t = \lambda_a^t /
                \Phi^t$
            \item Compound loss
                \begin{equation*}
                    \lambda_a^{t+1} = \lambda_a^t (1 -
                    \epsilon~\ell_a^t),~\forall a\in\mathcal{A}
                \end{equation*}
        \end{itemize}
        where $\Phi^t = \sum_a \lambda_a^t$
    \end{block}

\end{frame}

\begin{frame}
    \frametitle{Multiplicative weights algorithm}

    \begin{block}{Expected loss per round}
        \begin{equation*}
            L^t = \mathbb{E}(\ell^t) = \sum_a p_a^t \ell_a^t
        \end{equation*}
    \end{block}

    \begin{block}{Expected loss}
        \begin{equation*}
            L = \sum_{t} L^t
        \end{equation*}
    \end{block}

    Let's prove it!

\end{frame}

\subsection{Proving the loss bound}
\begin{frame}
    \frametitle{$\Phi$: a potential function argument}

    \begin{block}{Comparing $\Phi^t$ and $L^t$}
    \begin{equation*}
    \Phi^t = \sum_a \lambda_a^t
    \end{equation*}

    \begin{equation*}
    \Phi^{t+1}  = \Phi^t - \epsilon \sum_a \lambda_a^t \ell_a^t
    \end{equation*}

    Recall that $\lambda_a^t = \Phi^t p_a^t$

    \begin{equation*}
    \Phi^{t+1} = \Phi^t - \epsilon~\Phi^t \sum_a p_a^t \ell_a^t
    \end{equation*}

    \begin{equation*}
    \Phi^{t+1} = \Phi^t (1 - \epsilon L^t)
    \end{equation*}
    \end{block}

    \begin{block}{Taylor expansion}
    \begin{equation*}
        \Phi^{t+1} \leq \Phi^t \mathrm{e}^{- \epsilon L^t}
    \end{equation*}
    \end{block}

\end{frame}

\begin{frame}
    \frametitle{An upper bound on the potential}

    \begin{block}{By simple induction}
    Given
    \begin{equation*}
        \Phi^1 = \sum_a 1 = |\mathcal{A}|
    \end{equation*}
    and
    \begin{equation*}
        \Phi^{t+1} \leq \Phi^t \mathrm{e}^{- \epsilon L^t}
    \end{equation*}

    Let $L$ be the expected loss after $T$ rounds:
    \alert{\begin{equation*}
            \Phi^{T+1} \leq |\mathcal{A}| \mathrm{e}^{- \epsilon L}
    \end{equation*}}
    \end{block}

\end{frame}

\begin{frame}
    \frametitle{A lower bound on the compound loss}

    \begin{block}{Definition of the compound loss}
    For each action $a$,
    \begin{itemize}
        \item Initialization
            \begin{equation*}
                \lambda_a^1 = 1
            \end{equation*}
        \item Recurrence
            \begin{equation*}
                \lambda_a^{t+1} = \lambda_a^t (1 -
                \epsilon~\ell_a^t)
            \end{equation*}
    \end{itemize}

    \begin{equation*}
    \lambda_a^{T+1} = \prod_t^T (1 - \epsilon~\ell_a^t)
    \end{equation*}
    \end{block}

    \begin{block}{More Taylor expansions}
    \begin{equation*}
        (1 + \epsilon)^{-x} \leq (1 - \epsilon x), ~ x \in[-1, 0]
    \end{equation*}
    \begin{equation*}
        (1 - \epsilon)^x \leq (1 - \epsilon x), ~ x \in[0, 1]
    \end{equation*}
    \end{block}

\end{frame}

\begin{frame}
    \frametitle{A lower bound on the compound loss}


    \begin{block}{More Taylor expansions}
    \begin{equation*}
        (1 + \epsilon)^{-x} \leq (1 - \epsilon x), ~ x \in[-1, 0]
    \end{equation*}
    \begin{equation*}
        (1 - \epsilon)^x \leq (1 - \epsilon x), ~ x \in[0, 1]
    \end{equation*}
    \end{block}

    \begin{block}{Lower bounding $\lambda$}
    \alert{\begin{equation*}
        (1 + \epsilon)^{- \sum_{< 0} \ell_a^t}
        (1 - \epsilon)^{\sum_{\geq 0} \ell_a^t}
        \leq \lambda_a^{T+1}
    \end{equation*}}
    \end{block}

    \begin{block}{Note}
        If we assume that $\epsilon < \frac{1}{2}$ then $\lambda_a^t > 0$
    \end{block}

\end{frame}

\begin{frame}
    \frametitle{The potential is higher than each compound loss}

    \begin{block}{More Taylor expansions}
        Since for each action $a$ we have $\lambda_a^t > 0$
    \begin{equation*}
        \lambda_a^{T+1} \leq \sum_a \lambda_a^{T+1} = \Phi^{T+1}
    \end{equation*}
    \end{block}

    \begin{block}{Putting it all together}
        \begin{equation}
            (1 + \epsilon)^{- \sum_{< 0} \ell_a^t}
            (1 - \epsilon)^{\sum_{\geq 0} \ell_a^t}
            \leq \lambda_a^{T+1}
        \end{equation}
        \begin{equation}
            \lambda_a^{T+1} \leq \Phi^{T+1}
        \end{equation}
        \begin{equation}
            \Phi^{T+1} \leq |\mathcal{A}| \mathrm{e}^{- \epsilon L}
        \end{equation}
    \end{block}

    \alert{\begin{equation*}
            (1 + \epsilon)^{- \sum_{< 0} \ell_a^t}
            (1 - \epsilon)^{\sum_{\geq 0} \ell_a^t}
            \leq |\mathcal{A}| \mathrm{e}^{- \epsilon L}
    \end{equation*}}

\end{frame}

\begin{frame}
    \frametitle{Wrapping it up}

    \begin{block}{Bounding $L$}
        \begin{equation*}
                    (1 + \epsilon)^{- \sum_{< 0} \ell_a^t}
                    (1 - \epsilon)^{\sum_{\geq 0} \ell_a^t}
                    \leq |\mathcal{A}| \mathrm{e}^{- \epsilon L}
            \end{equation*}
            Taking the $\ln$ gives us:
        \begin{equation*}
        - \sum_{<0} \ell_a^t \ln(1 + \epsilon)
        + \sum_{\geq 0} \ell_a^t \ln(1 - \epsilon)
        \leq \ln |\mathcal{A}| -\epsilon L
        \end{equation*}

        \begin{equation*}
        L \leq
        \sum_{< 0} \ell_a^t \frac{\ln(1 + \epsilon)}{\epsilon}
        - \sum_{\geq 0} \ell_a^t \frac{\ln(1 - \epsilon)}{\epsilon}
        +\frac{\ln |\mathcal{A}|}{\epsilon}
        \end{equation*}
    \end{block}

    \begin{block}{Our first second-order Taylor expansions}
        \begin{equation*}
            \ln(1 + \epsilon) \geq \epsilon - \epsilon^2
        \end{equation*}
        \begin{equation*}
        -\ln(1 - \epsilon) \leq \epsilon + \epsilon^2
        \end{equation*}
    \end{block}

\end{frame}

\begin{frame}
    \frametitle{Wrapping it up}

    Using
    $\frac{\ln(1 + \epsilon)}{\epsilon} \geq 1 - \epsilon$ and
    $-\frac{\ln(1 - \epsilon)}{\epsilon} \leq 1 + \epsilon$
    we get:

    \begin{block}{Done!}
    \begin{align*}
    L & \leq
        \frac{1}{\epsilon} \sum_{< 0} \ell_a^t (1 - \epsilon)
        + \sum_{\geq 0} \ell_a^t (1 + \epsilon)
    + \frac{\ln |\mathcal{A}|}{\epsilon} \\
    L & \leq
    \sum_t \ell_a^t
    + \epsilon
    \underbrace{
    (- \sum_{< 0} \ell_a^t + \sum_{\geq 0} \ell_a^t)}_{\sum_{t} |\ell_a^t|}
    + \frac{\ln |\mathcal{A}|}{\epsilon} \\
    \end{align*}

    With $|L_a| = \sum_{t} |\ell_a^t|$, we obtain:
    \begin{equation*}
    L \leq L_a + \epsilon~|L_a|
    +\frac{\ln |\mathcal{A}|}{\epsilon},~ \forall a\in \mathcal{A}
    \end{equation*}
    \end{block}

\end{frame}

\begin{frame}
    \frametitle{Regret}

    \begin{block}{What does the bound on $L$ mean?}
    \begin{equation*}
    L \leq L_a + \epsilon~|L_a|
    +\frac{\ln |\mathcal{A}|}{\epsilon},~ \forall a\in \mathcal{A}
    \end{equation*}

    In particular, with regard to the best action in hindsight:

    \begin{equation*}
    L \leq L_* + \epsilon~|L_*| +\frac{\ln |\mathcal{A}|}{\epsilon}
    \end{equation*}
    \end{block}

    \begin{block}{Average expected regret}
        Over $T$ rounds, we define the average expected regret by:
        \begin{equation*}
            R = \frac{L - L_*}{T}
        \end{equation*}
    \end{block}

\end{frame}

\begin{frame}
    \frametitle{Not feeling much regret}

    \begin{block}{Regret bound}
        Rewriting $|L_*|$ as $||\ell_{a^*}||_1$, we get:
    \begin{equation*}
    L - L_* \leq \epsilon~||\ell_{a^*}||_1 + \frac{\ln |\mathcal{A}|}{\epsilon}
    \end{equation*}
    The trivial norm bound $||\ell_{a^*}||_1 \leq T$ gives:
    \begin{equation*}
    L - L_* \leq \epsilon T + \frac{\ln |\mathcal{A}|}{\epsilon}
    \end{equation*}

    \begin{equation*}
    R \leq \epsilon + \frac{\ln |\mathcal{A}|}{\epsilon T}
    \end{equation*}
    \end{block}

    \begin{block}{How long until I feel no regret?}
        \begin{equation*}
            T = \frac{\ln{|\mathcal{A}|}}{\epsilon^2} \implies R \leq 2
            \epsilon
        \end{equation*}

    \end{block}

\end{frame}

\section{Applications}
\begin{frame}
    \frametitle{Applications}

    \begin{block}{Wide variety of domains}
        \begin{itemize}
            \item Stock market prediction
            \item Online Convex Optimization
            \item Boosting (Ensemble Learning)
            \item Approximate Linear Optimization
            \item Multicommodity flow problems $\leftarrow$ \alert{today}
            \item and many more! competitive online algorithms, SDP,
                approximation algorithms for NP-hard problems, \ldots
        \end{itemize}
    \end{block}

    \begin{block}{Applicable yes but efficient?}
        Analysis of the algorithm is tight.

        Given the same assumptions, no
        algorithm produces better bounds.
    \end{block}

\end{frame}

\subsection{Multicommodity flow}
\begin{frame}
    \frametitle{The multicommodity flow problem}

    Given a capacitated graph together with a set of origin-destination pairs
    (representing a source and a target).

    \begin{block}{Problem}
        Find the maximum amount of flow between origin-destination pairs that
        satisfies capacity constraints
    \end{block}

    \begin{block}{Complexity}
        \begin{itemize}
            \item If the flow is integral, NP-hard.
            \item If the flow is fractional, can be cast as a large linear
                program so it is $\in$ P. $\leftarrow$ \alert{today}
            \item If there is only one origin-destination pair, reduces to
                the simple maximum flow.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{The maximum multicommodity flow as a LP}
    \framesubtitle{A path formulation}

    \begin{align*}
    (\Pi) & \begin{dcases}
    \max~ & \sum_{p\in\mathcal{P}} f_p\\
    \text{s.t.}~& \sum_{p\ni a} f_p \leq c_a,~\forall a\in A ~(l_a)\\
    & f \in \mathbb{R}_+^{|\mathcal{P}|}
    \end{dcases}
    \\
    (\Delta) & \begin{dcases}
    \min~ & \sum_{a\in A} c_a l_a\\
    \text{s.t.}~& \sum_{a\in p} l_a \geq 1,~\forall p\in \mathcal{P} ~(f_p)\\
    & l \in \mathbb{R}_+^{|A|}
    \end{dcases}
    \end{align*}

\end{frame}

\begin{frame}
    \frametitle{Learning from expert arcs}

    \begin{block}{Using the gain guarantee}
        \begin{equation*}
            G \geq G_a - \epsilon~|G_a| - \dfrac{\ln |\mathcal{A}|}{\epsilon},
            ~\forall a\in \mathcal{A}
        \end{equation*}
    \end{block}

    \begin{block}{Mapping concepts}
    \begin{tabular}{r | c c c | l}
    actions & $\mathcal{A}$ & $\to$ & $A$ & arcs\\
    gain & $g_a$ & $\to$ & $f_a / c_a$ & network utilization\\
    compound gain & $l_a$ & $\to$ & $l_a$ & length (compound congestion)
    \end{tabular}
    \end{block}

\end{frame}

\begin{frame}
    \frametitle{Bounding network utilization}

    Can $f_a / c_a$ be bounded?
    \begin{block}{Yes!}
        If the flow routed at each round does not exceed the minimum capacity
        of the path.
    \end{block}

    \begin{block}{We can apply the multiplicative weights algorithm}
        \begin{itemize}
            \item $A$: finite set of arcs
            \item $g_a = f_a / c_a \in [0,1]$
        \end{itemize}
    \end{block}

\end{frame}

\subsection{Designing a flow algorithm}
\begin{frame}
    \frametitle{A primal-dual algorithm based on multiplicative weights}

    \begin{block}{Initialization}
        \begin{itemize} \item
            For each arc $a\in A$, set length $l_a^1 = 1$
        \end{itemize}
    \end{block}
    \begin{block}{At round $t$}
        \begin{itemize}
        \item Compute the shortest path $p^t$ relative to the length $l^t$
            among all paths in $\mathcal{P}$ and choose all arcs $a$ on that
            path.
        \item The gain on each arc is given relative to the flow that saturates
             $p^t$:
            \begin{equation*}
            f^t = \min_{a\in p^*} c_a
            \end{equation*}
            \begin{equation*}
                g_a^t = \left\{
        \begin{array}{l l}
        f^t/c_a & \text{if $a \in p^t$}\\
        0 & \text{otherwise}
        \end{array}
        \right.
            \end{equation*}

        \item Compound gain
            \begin{equation*}
            l_a^{t+1} = l_a^t (1 + \epsilon~g_a^t)
            \end{equation*}
        \end{itemize}
    \end{block}

\end{frame}

\subsection{Proving it correct}
\begin{frame}
    \frametitle{Analysis}

    \begin{block}{Relating $G_*$ and the maximum network utilization}
        Since $g_a^t \in [0,1]$:
        \begin{equation*}
            G \geq (1 - \epsilon) G_* - \dfrac{\ln |A|}{\epsilon}
        \end{equation*}
        with
        \begin{equation*}
            G_* = \max_{a\in A} \sum_t f^t_a / c_a
        \end{equation*}
        Which means that $G_*$ is the value of the edge with maximum
        utilization in the network.
    \end{block}

\end{frame}

\begin{frame}
    \frametitle{Analysis}

    \begin{block}{Relating $G$ and the approximation}
        \begin{equation*}
            G \geq (1 - \epsilon) G_* - \dfrac{\ln |A|}{\epsilon}
        \end{equation*}

        We set the probabilities $p_a^t = l_a^t / \Phi^t$ a posteriori, which
        gives:
        \begin{equation*}
            G = \sum_t \sum_{a\in A} g_a^t \frac{l_a^t}{\Phi^t}
        \end{equation*}
        \begin{equation*}
            G = \sum_t \sum_{a\in p^t} g_a^t \frac{l_a^t}{\Phi^t}
        \end{equation*}
        \begin{equation*}
            G = \sum_t f^t \frac{\sum_{a\in p^t} l_a^t / c_a}{\Phi^t}
        \end{equation*}
    \end{block}

\end{frame}

\begin{frame}
    \frametitle{Upper bounded by the optimal flow}

    \begin{block}{Relevant facts}
        Abstracting away $t$, we have:
        \begin{equation*}
            \frac{\sum_{a\in p} l_a / c_a}{\Phi}
        \end{equation*}
        Consider the following:
        \begin{itemize}
            \item $p^*$ is a shortest path w.r.t. $l_a / c_a$
            \item $F^* = \sum_q f_q^*$ is an optimal (feasible) flow
        \end{itemize}
    \end{block}

    \begin{block}{Relating $G$ and the approximation}
\begin{equation*}
\dfrac{\sum_{a \in p^*} l_a / c_a}{\sum_a l_a}
\leq
\dfrac{\sum_{a \in p^*} l_a / c_a}
{\sum_a l_a \sum_{q\ni a} \frac{f_q^*}{c_a}}
=
\dfrac{\sum_{a \in p^*} l_a / c_a}
{\sum_q f_q^* \sum_{a\in q} l_a / c_a}
\end{equation*}
    \end{block}

\end{frame}

\begin{frame}
    \frametitle{Upper bounded by the optimal flow}

    \begin{block}{Relating $G$ and the approximation}
\begin{equation*}
\dfrac{1}{ \sum_{q} f_q^*}
\cdot
\dfrac{\sum_{a \in p^*} l_a / c_a}
{\sum_{a\in q} l_a / c_a}
=
\dfrac{1}{F^*}
\cdot
\dfrac{\sum_{a \in p^*} l_a / c_a}
{\sum_{a\in q} l_a / c_a}
\leq
\dfrac{1}{F^*}
\end{equation*}
which gives the following bound:
\begin{equation*}
\dfrac{\sum_{a \in p^t} l_a^t / c_a}{\Phi^t}
\leq
\dfrac{1}{F^*},~\forall t \label{bound}
\end{equation*}
    \end{block}

\end{frame}

\begin{frame}
    \frametitle{Upper bounded by the optimal flow}


    \begin{block}{Relating $G$ and the approximation}
    If we call $F = \sum_t f^t$ the flow given by our algorithm:
    \begin{equation*}
    G = \sum_t f^t \dfrac{\sum_{a \in p^t} l_a^t / c_a}{\Phi^t}
    \leq
    \sum_t f^t ~ \dfrac{1}{F^*}
    = \dfrac{F}{F^*} \label{approx-bound}
    \end{equation*}
    \end{block}

    \begin{equation*}
    \dfrac{F}{F^*} \geq G \geq (1 - \epsilon) G_* - \frac{\ln|A|}{\epsilon}
    \end{equation*}
\end{frame}

\begin{frame}
    \frametitle{Gain guarantee $\to$ Approximation guarantee}

    \begin{block}{Scaling}
    Now we can scale down our flow by $G_*$ to obtain $\hat{F} = F / G_*$:
    \begin{equation*}
    \dfrac{\hat{F}}{F^*} \geq (1 - \epsilon) - \frac{\ln|A|}{\epsilon C}
    \end{equation*}
    \end{block}

    \begin{block}{Approximation guarantee}
    \begin{equation*}
        G_* = \frac{\ln|A|}{\epsilon^2}
    \end{equation*}
    implies that
    \begin{equation*}
    \hat{F} \geq (1 - 2\epsilon) F^*
    \end{equation*}
    our algorithm returns a solution with arbitrary precision.
    \end{block}

\end{frame}

\begin{frame}
    \frametitle{In polynomial time?}

    \begin{block}{FPTAS}
        An algorithm returning a solution with arbitrary precision $\epsilon$
        \begin{itemize}
            \item in polynomial time w.r.t the instance size
            \item in polynomial time w.r.t the precision $\epsilon^{-1}$
        \end{itemize}
    \end{block}

    \begin{block}{How many iterations?}
        \begin{itemize}
            \item Each edge can receive congesting flow at most
                $O(\ln|A|~\epsilon^{-2})$ times.
            \item $|A|$ edges in total
        \end{itemize}
        \begin{equation*}
        O(m\log m~\epsilon^{-2})~\text{iterations.}
        \end{equation*}
    \end{block}

    \begin{block}{Cost per iteration?}
        The cost of finding $k$ shortest paths where $k$ is the number of
        origin-destination pairs:
        \begin{equation*}
            O(km)
        \end{equation*}
    \end{block}


\end{frame}

\begin{frame}
    \frametitle{Conclusion}

    \begin{block}{Total runtime}
    \begin{equation*}
    O(km^2\log m~\epsilon^{-2})
    \end{equation*}
    \end{block}

    \begin{block}{Application of the multiplicative weights}
        Practical benefits:
        \begin{itemize}
            \item simple implementation (16 lines of Python)
            \item very fast for $\epsilon \approx 10^{-1}$
            \item simple analysis
            \item adaptable to any convex setting!
        \end{itemize}
    \end{block}

\end{frame}

\begin{frame}
    \frametitle{Any questions?}

    \begin{center}
        \huge{Thank you!}
    \end{center}
\end{frame}

\begin{frame}
    \frametitle{Applications}

    \begin{block}{Wide variety of domains}
        \begin{itemize}
            \item Stock market prediction $\leftarrow$ \alert{today?}
            \item \ldots
        \end{itemize}
    \end{block}

    \begin{block}{Setting}
        \begin{itemize}
            \item $\mathcal{A}$: financial experts predicting \{up, down\}
            \item $\ell_a^t = \left\{
        \begin{array}{l l}
        1 & \text{if expert $a$ got it wrong at round $t$}\\
        0 & \text{otherwise}
        \end{array}
        \right.$
    \item $L$, $L_a$, $L_*$: (expected) number of mistakes
\end{itemize}
    \end{block}

    \begin{block}{Result}
        \begin{equation*}
            L \leq (1 + \epsilon)L_* + \dfrac{\ln |\mathcal{A}|}{\epsilon}
        \end{equation*}
    \end{block}
    

\end{frame}

\begin{frame}
    \frametitle{Also available in $[-\rho, \rho]$!}

    \begin{block}{Weaker bound on the loss}
        \begin{equation*}
            \ell_a^t \in [-\rho, \rho]
        \end{equation*}
    \end{block}

    \begin{block}{How long until I feel no regret?}
        Roughly, to achieve arbitrary regret of $\epsilon$:
        \begin{equation*}
        O(\rho^2 \ln |\mathcal{A}|~\epsilon^{-2}) \text{iterations}
        \end{equation*}
        and your runtime per iteration might depend on the width too!
    \end{block}

\end{frame}

\begin{frame}
    \frametitle{Bibliography}

    \begin{thebibliography}{9}
        \bibitem{A01} Sanjeev Arora, Elad Hazan and Satyen Kale \newblock
            ``The Multiplicative Weights Update Method:\\ A Meta-Algorithm and
            Applications''. \newblock Theory of Computing 2012.
    \end{thebibliography}
\end{frame}

\end{document}
